{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=True, random_state=42)\n",
    "# train transformer on train data, and transform them\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# now the transformer is trained on train data, it can be applied on test data\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression()",
      "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.36008828e-01, 8.63991172e-01],\n       [9.99977295e-01, 2.27049622e-05],\n       [9.96080057e-01, 3.91994328e-03],\n       [7.80003100e-04, 9.99219997e-01],\n       [1.14294099e-04, 9.99885706e-01],\n       [1.00000000e+00, 3.50087563e-10],\n       [9.99999993e-01, 7.08653260e-09],\n       [9.54663200e-01, 4.53368002e-02],\n       [4.31753581e-01, 5.68246419e-01],\n       [1.14096940e-03, 9.98859031e-01],\n       [5.99262279e-02, 9.40073772e-01],\n       [9.84076010e-01, 1.59239901e-02],\n       [7.18535007e-03, 9.92814650e-01],\n       [8.28290034e-01, 1.71709966e-01],\n       [2.59614386e-03, 9.97403856e-01],\n       [9.98448510e-01, 1.55149008e-03],\n       [2.75117091e-03, 9.97248829e-01],\n       [1.59757689e-05, 9.99984024e-01],\n       [1.28262413e-06, 9.99998717e-01],\n       [9.99997173e-01, 2.82651584e-06],\n       [8.40555409e-02, 9.15944459e-01],\n       [1.21140991e-02, 9.87885901e-01],\n       [9.99999984e-01, 1.61112524e-08],\n       [1.22092033e-04, 9.99877908e-01],\n       [1.81219116e-03, 9.98187809e-01],\n       [7.30733967e-04, 9.99269266e-01],\n       [1.90748256e-03, 9.98092517e-01],\n       [1.04990162e-02, 9.89500984e-01],\n       [4.07086386e-03, 9.95929136e-01],\n       [9.99979610e-01, 2.03903732e-05],\n       [1.10679302e-03, 9.98893207e-01],\n       [2.37967148e-04, 9.99762033e-01],\n       [2.82608362e-03, 9.97173916e-01],\n       [6.44288393e-03, 9.93557116e-01],\n       [2.12572906e-04, 9.99787427e-01],\n       [3.64640611e-03, 9.96353594e-01],\n       [9.13640398e-01, 8.63596017e-02],\n       [5.46281913e-03, 9.94537181e-01],\n       [9.99826151e-01, 1.73849052e-04],\n       [4.79134206e-02, 9.52086579e-01],\n       [1.36941225e-04, 9.99863059e-01],\n       [9.99423415e-01, 5.76585410e-04],\n       [4.67052429e-03, 9.95329476e-01],\n       [1.44849620e-03, 9.98551504e-01],\n       [2.61904313e-02, 9.73809569e-01],\n       [8.66124916e-02, 9.13387508e-01],\n       [5.94044359e-04, 9.99405956e-01],\n       [4.84142724e-04, 9.99515857e-01],\n       [3.96472446e-02, 9.60352755e-01],\n       [1.54327807e-03, 9.98456722e-01],\n       [9.99806906e-01, 1.93094125e-04],\n       [9.99999849e-01, 1.51230542e-07],\n       [2.97289526e-01, 7.02710474e-01],\n       [2.45159314e-02, 9.75484069e-01],\n       [4.15295626e-05, 9.99958470e-01],\n       [1.31540513e-02, 9.86845949e-01],\n       [2.02687219e-04, 9.99797313e-01],\n       [1.00000000e+00, 5.36833422e-11],\n       [8.55570997e-01, 1.44429003e-01],\n       [2.49386825e-04, 9.99750613e-01],\n       [6.60323131e-03, 9.93396769e-01],\n       [9.99999001e-01, 9.98834078e-07],\n       [9.99999995e-01, 4.76325232e-09],\n       [3.99820878e-02, 9.60017912e-01],\n       [1.02304313e-03, 9.98976957e-01],\n       [1.61858395e-01, 8.38141605e-01],\n       [9.99977357e-01, 2.26431543e-05],\n       [9.99999994e-01, 6.27751846e-09],\n       [1.08344834e-03, 9.98916552e-01],\n       [3.08258071e-02, 9.69174193e-01],\n       [9.98311923e-01, 1.68807750e-03],\n       [9.97407848e-01, 2.59215178e-03],\n       [3.39337935e-03, 9.96606621e-01],\n       [9.95475734e-01, 4.52426604e-03],\n       [1.01609704e-04, 9.99898390e-01],\n       [1.29838994e-02, 9.87016101e-01],\n       [1.98238296e-02, 9.80176170e-01],\n       [5.21797974e-01, 4.78202026e-01],\n       [4.97747550e-05, 9.99950225e-01],\n       [2.10208843e-02, 9.78979116e-01],\n       [9.93778974e-01, 6.22102571e-03],\n       [9.65829459e-05, 9.99903417e-01],\n       [8.23746234e-01, 1.76253766e-01],\n       [9.99999979e-01, 2.05481990e-08],\n       [9.92620298e-01, 7.37970207e-03],\n       [9.98525239e-01, 1.47476101e-03],\n       [9.99824146e-01, 1.75853647e-04],\n       [9.99752260e-01, 2.47740217e-04],\n       [4.35760888e-04, 9.99564239e-01],\n       [4.69140632e-03, 9.95308594e-01],\n       [8.87960208e-03, 9.91120398e-01],\n       [3.31466871e-01, 6.68533129e-01],\n       [3.29027876e-02, 9.67097212e-01],\n       [2.54214671e-04, 9.99745785e-01],\n       [4.22736174e-04, 9.99577264e-01],\n       [1.87080326e-04, 9.99812920e-01],\n       [9.99998220e-01, 1.77985194e-06],\n       [9.99997746e-01, 2.25398808e-06],\n       [1.44916781e-04, 9.99855083e-01],\n       [9.99526403e-01, 4.73597374e-04],\n       [9.95115290e-01, 4.88470996e-03],\n       [2.99746235e-06, 9.99997003e-01],\n       [9.99960531e-01, 3.94692783e-05],\n       [9.98994155e-01, 1.00584504e-03],\n       [1.15900284e-02, 9.88409972e-01],\n       [4.19047603e-02, 9.58095240e-01],\n       [6.49164583e-03, 9.93508354e-01],\n       [9.99999999e-01, 7.16824191e-10],\n       [1.56767742e-01, 8.43232258e-01],\n       [3.76729953e-02, 9.62327005e-01],\n       [9.99089117e-01, 9.10883147e-04],\n       [2.24996714e-03, 9.97750033e-01],\n       [5.22806001e-01, 4.77193999e-01],\n       [1.00000000e+00, 9.08091808e-12],\n       [6.66097189e-01, 3.33902811e-01],\n       [9.99999990e-01, 9.81926521e-09],\n       [1.21861921e-05, 9.99987814e-01],\n       [4.90515628e-02, 9.50948437e-01],\n       [4.59071838e-05, 9.99954093e-01],\n       [9.99873808e-01, 1.26192032e-04],\n       [5.75599574e-04, 9.99424400e-01],\n       [7.35475712e-04, 9.99264524e-01],\n       [7.63905339e-03, 9.92360947e-01],\n       [9.99821018e-01, 1.78982448e-04],\n       [9.47080840e-02, 9.05291916e-01],\n       [9.99999410e-01, 5.90284980e-07],\n       [9.99261840e-01, 7.38160421e-04],\n       [1.65033896e-03, 9.98349661e-01],\n       [2.01392059e-03, 9.97986079e-01],\n       [9.99996345e-01, 3.65513454e-06],\n       [9.99999976e-01, 2.39736318e-08],\n       [9.99997054e-01, 2.94567827e-06],\n       [1.07836397e-02, 9.89216360e-01],\n       [2.25101319e-03, 9.97748987e-01],\n       [2.29761207e-02, 9.77023879e-01],\n       [9.95889653e-01, 4.11034657e-03],\n       [2.67263429e-01, 7.32736571e-01],\n       [1.00751684e-02, 9.89924832e-01],\n       [2.33991540e-01, 7.66008460e-01],\n       [9.98899199e-01, 1.10080064e-03],\n       [2.83000059e-03, 9.97169999e-01],\n       [9.99999980e-01, 1.95866746e-08],\n       [5.18552443e-05, 9.99948145e-01],\n       [5.19879273e-05, 9.99948012e-01],\n       [9.96464681e-01, 3.53531925e-03],\n       [1.29805038e-03, 9.98701950e-01],\n       [9.99992968e-01, 7.03242601e-06],\n       [9.99997428e-01, 2.57207683e-06],\n       [6.49507453e-01, 3.50492547e-01],\n       [2.35555518e-04, 9.99764444e-01],\n       [9.32152671e-01, 6.78473287e-02],\n       [2.10200101e-04, 9.99789800e-01],\n       [1.72543066e-05, 9.99982746e-01],\n       [2.56101443e-02, 9.74389856e-01],\n       [1.39995366e-03, 9.98600046e-01],\n       [1.00000000e+00, 2.06233933e-12],\n       [9.99845224e-01, 1.54775982e-04],\n       [8.02039877e-05, 9.99919796e-01],\n       [1.05404150e-02, 9.89459585e-01],\n       [2.92748127e-05, 9.99970725e-01],\n       [1.27416370e-07, 9.99999873e-01],\n       [7.18501082e-04, 9.99281499e-01],\n       [2.97626894e-04, 9.99702373e-01],\n       [3.69568010e-03, 9.96304320e-01],\n       [9.19483625e-01, 8.05163752e-02],\n       [1.38787382e-03, 9.98612126e-01],\n       [1.11998521e-03, 9.98880015e-01],\n       [3.44182878e-01, 6.55817122e-01],\n       [5.11538299e-05, 9.99948846e-01],\n       [9.82492050e-01, 1.75079499e-02],\n       [2.76265625e-02, 9.72373437e-01],\n       [1.55637430e-03, 9.98443626e-01],\n       [1.33895698e-03, 9.98661043e-01],\n       [9.42844934e-02, 9.05715507e-01],\n       [3.48799565e-03, 9.96512004e-01],\n       [3.68918826e-02, 9.63108117e-01],\n       [8.53150688e-01, 1.46849312e-01],\n       [4.74571262e-02, 9.52542874e-01],\n       [2.75672918e-03, 9.97243271e-01],\n       [2.01980779e-02, 9.79801922e-01],\n       [9.13203419e-02, 9.08679658e-01],\n       [8.56655564e-02, 9.14334444e-01],\n       [5.28127554e-04, 9.99471872e-01],\n       [9.84826268e-01, 1.51737317e-02],\n       [9.99682603e-01, 3.17396737e-04],\n       [9.50134564e-01, 4.98654363e-02],\n       [5.86382722e-01, 4.13617278e-01],\n       [8.49599115e-02, 9.15040088e-01]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = lr.predict_proba(X_test)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for evaluating fp,tp and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fp_tp(actual, predicted):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(actual, predicted):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fp,tp and accuracy evaluations for different thresholds\n",
    "\n",
    "Given the scores for the test cases, we might want to find the best possible threshold for classification, i.e., the real value $t$ such that `scores >` $t$ gives the best classifiation of the examples. \n",
    "\n",
    "Let us then start to consider 100 possible thresholds in the range $[0,1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and compute the tp, fp, and accuracy values of the labelings obtained by comparing the scores with those thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances = []\n",
    "fps, tps = [], []\n",
    "#loop in cui mettiamo i falsi positivi e i falsi negativi\n",
    "...\n",
    "\n",
    "# performances [(acc, t, fp, tp)]\n",
    "\n",
    "performances = np.array(performances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting\n",
    "\n",
    "Let us then start plotting the coverage plot for the obtained classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fps, tps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking performances for threshold 0.5\n",
    "\n",
    "The predict_proba method we used to get the score returns the probability that examples belong to the positive class. Usually the positive class is then predicted as score > 0.5 (since in this case it is the one with the largest likelihood).\n",
    "\n",
    "Let's then see where this classifier (i.e., the one obtained setting the threshold to 0.5) lays in the coverage plot and if there are better options.\n",
    "\n",
    "**note**: since we saved interesting stats in the `performances` array, we can retrieve the fp, tp position of the classifier we get by setting the thresholds to 0.5, by finding the position of the row we are interested using the expression: `performances[:,1] == 0.5` and then using the resulting boolean vector to retrieve the correct row of the matrix: `performances[performances[:,1] == 0.5]`.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fps, tps)\n",
    "accuracy, threshold, fp, tp = performances[performances[:,1] == 0.5][0]\n",
    "plt.scatter(fp,tp,color='red')\n",
    "plt.plot([fp-10,fp+10],[tp-10,tp+10], color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is shown by the red dot and the red line, threshold 0.5 is a good one, but apparently two other points can reach a better classification.\n",
    "\n",
    "Let us see where these point lay in the plot and what is their accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the subscription returns a matrix with a single row, but still two dimensions, \n",
    "# we need to get the element in the first position of that matrix...\n",
    "\n",
    "perf05 = performances[performances[:, 1] == 0.5][0,0] \n",
    "performances[performances[:,0] > perf05]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two points that we are looking for are then in position (5,121) and (1,117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fps, tps)\n",
    "fp, tp = eval_fp_tp(actual, scores > 0.5)\n",
    "plt.scatter(fp,tp, color=\"red\")\n",
    "plt.scatter(5,121, color=\"orange\")\n",
    "plt.scatter(1,117, color=\"orange\")\n",
    "plt.plot([fp-10,fp+10],[tp-10,tp+10], color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two points (that we found by looking only to the accuracies) are indeed the two points that the plot show having a better accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "9c92e2d91347891a775597460ced9ced45550a53bd4d3e0bf3522d584ebb9c2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
